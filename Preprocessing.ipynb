{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7762c882",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â–¶ Lemmatization only:\n",
      "['hello', 'testing', 'nlp', 'pipeline', 'using', 'sample', 'url', 'along', 'contraction', 'like', 'number', '123']\n",
      "\n",
      "â–¶ Stemming only:\n",
      "['hello', 'test', 'nlp', 'pipelin', 'use', 'sampl', 'url', 'along', 'contract', 'like', 'number', '123']\n",
      "\n",
      "â–¶ Both Lemmatization + Stemming:\n",
      "['hello', 'test', 'nlp', 'pipelin', 'use', 'sampl', 'url', 'along', 'contract', 'like', 'number', '123']\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "ðŸ“Œ NLP Preprocessing Pipeline (Reusable Python Template)\n",
    "---------------------------------------------------------\n",
    "This script provides a general-purpose text preprocessing pipeline for common NLP tasks, including:\n",
    "- Text classification\n",
    "- Sentiment analysis\n",
    "- Topic modeling\n",
    "- Named entity recognition\n",
    "\n",
    "You can choose whether to apply:\n",
    "- Lemmatization\n",
    "- Stemming\n",
    "- Both\n",
    "- Or none\n",
    "\n",
    "Usage:\n",
    "------\n",
    "Import `preprocess_text()` into your project, or run this script directly to test on a sample text.\n",
    "\"\"\"\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "import contractions\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "\n",
    "# ðŸ”§ Download required NLTK resources (only first time)\n",
    "nltk.download(\"punkt\", quiet=True)\n",
    "nltk.download(\"stopwords\", quiet=True)\n",
    "nltk.download(\"wordnet\", quiet=True)\n",
    "\n",
    "# âš™ï¸ Initialize tools\n",
    "_stopwords = set(stopwords.words(\"english\"))\n",
    "_lemmatizer = WordNetLemmatizer()\n",
    "_stemmer = PorterStemmer()\n",
    "\n",
    "\n",
    "def preprocess_text(text: str, lemmatize=True, stem=False) -> list:\n",
    "    \"\"\"\n",
    "    Preprocesses raw input text through a standard NLP pipeline.\n",
    "\n",
    "    Steps:\n",
    "        1. Remove HTML tags\n",
    "        2. Convert to lowercase\n",
    "        3. Expand contractions\n",
    "        4. Remove URLs\n",
    "        5. Remove punctuation and special characters\n",
    "        6. Normalize whitespace\n",
    "        7. Tokenize text\n",
    "        8. Remove stopwords\n",
    "        9. Lemmatize or Stem tokens (based on user choice)\n",
    "\n",
    "    Args:\n",
    "        text (str): Raw input text.\n",
    "        lemmatize (bool): Whether to apply lemmatization.\n",
    "        stem (bool): Whether to apply stemming.\n",
    "\n",
    "    Returns:\n",
    "        list: List of clean tokens.\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. Remove HTML tags\n",
    "    text = re.sub(r\"<.*?>\", \"\", text)\n",
    "\n",
    "    # 2. Convert to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # 3. Expand contractions\n",
    "    text = contractions.fix(text)\n",
    "\n",
    "    # 4. Remove URLs\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text)\n",
    "\n",
    "    # 5. Remove punctuation and special characters\n",
    "    text = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", text)\n",
    "\n",
    "    # 6. Normalize extra whitespace\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "    # 7. Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # 8. Remove stopwords\n",
    "    tokens = [token for token in tokens if token not in _stopwords]\n",
    "\n",
    "    # 9. Lemmatize\n",
    "    if lemmatize:\n",
    "        tokens = [_lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "    # 10. Stem\n",
    "    if stem:\n",
    "        tokens = [_stemmer.stem(token) for token in tokens]\n",
    "\n",
    "    return tokens\n",
    "\n",
    "\n",
    "# ðŸš€ Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    sample_text = \"\"\"\n",
    "    <p>Hello there! I'm testing this NLP pipeline using a sample URL: https://example.com,\n",
    "    along with contractions like can't and numbers 123.</p>\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"â–¶ Lemmatization only:\")\n",
    "    print(preprocess_text(sample_text, lemmatize=True, stem=False))\n",
    "\n",
    "    print(\"\\nâ–¶ Stemming only:\")\n",
    "    print(preprocess_text(sample_text, lemmatize=False, stem=True))\n",
    "\n",
    "    print(\"\\nâ–¶ Both Lemmatization + Stemming:\")\n",
    "    print(preprocess_text(sample_text, lemmatize=True, stem=True))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
